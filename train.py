# train.py
import torch
import torch.nn.functional as F
import numpy as np
import random
import copy
from tqdm import trange
from torch.utils.tensorboard import SummaryWriter
from typing import Dict, List, Tuple, Optional
import os
from datetime import datetime
from dag import DAGTasks
from env import OffloadEnv
from agent import DoubleDQNAgent
from federated_learning import HierFL
from noise_config import (
    DELTA_SENS, EPSILON_TOTAL, DELTA_TOTAL,
    compute_level0_budgets, compute_sigma_range_for_episode,
    compute_dpsgd_noise_multiplier
)

_ALPHAS_DENSE = [1.0 + 0.1 * i for i in range(2, 100)]
_ALPHAS_WIDE = [12, 14, 16, 20, 32, 64, 128, 256, 512]
_ALPHAS_EXT = [float(a) for a in (_ALPHAS_DENSE + _ALPHAS_WIDE)]


class DPSigmaPredictor:
    """
    å·®åˆ†éšç§Sigmaé¢„æµ‹å™¨åŒ…è£…ç±»ï¼ˆåŸºäºæ ‡å‡† DP-SGDï¼‰
    
    åŠŸèƒ½ï¼š
    1. åŠ è½½é¢„è®­ç»ƒçš„Sigmaé¢„æµ‹å™¨
    2. ä½¿ç”¨DP-SGDè¿›è¡Œåœ¨çº¿å¾®è°ƒï¼ˆbatch_size=1ï¼‰
    3. ä¿ç•™è§„åˆ™åŸºçº¿ä½œä¸ºæ­£åˆ™åŒ–çº¦æŸ
    4. åˆ†é˜¶æ®µè°ƒæ•´å¾®è°ƒå¼ºåº¦
    
    """
    
    def __init__(self, 
                 pretrained_model,
                 epsilon: float,
                 delta: float,
                 total_steps: int,
                 learning_rate: float = 1e-6,
                 device: str = 'cpu',
                 params: Dict = None):
        """
        åˆå§‹åŒ–DP Sigmaé¢„æµ‹å™¨
        
        Args:
            pretrained_model: é¢„è®­ç»ƒçš„Sigmaé¢„æµ‹å™¨æ¨¡å‹
            epsilon: DP-SGDéšç§é¢„ç®—
            delta: DP-SGD deltaå‚æ•°
            total_steps: æ€»è®­ç»ƒæ­¥æ•°
            learning_rate: å­¦ä¹ ç‡
            device: è®¾å¤‡
            params: å…¶ä»–å‚æ•°
        """
        self.device = device
        self.params = params or {}
        
        # æ¨¡å‹
        self.model = pretrained_model.to(device)
        
        # ä¿å­˜é¢„è®­ç»ƒå‚æ•°ï¼ˆç”¨äºL2æ­£åˆ™åŒ–ï¼Œä¿æŒé¢„è®­ç»ƒç‰¹æ€§ï¼‰
        self.pretrained_params = {}
        for name, param in self.model.named_parameters():
            self.pretrained_params[name] = param.data.clone().detach()
        
        # DP-SGDå‚æ•°
        self.epsilon = epsilon
        self.delta = delta
        self.total_steps = total_steps
        self.current_step = 0
        self.max_grad_norm = 1.0  # æ¢¯åº¦è£å‰ªèŒƒæ•°ï¼ˆå¿…é¡»åœ¨_compute_noise_multiplierä¹‹å‰å®šä¹‰ï¼‰
        
        # è®¡ç®—å™ªå£°å€æ•°ï¼ˆä¾èµ–max_grad_normï¼‰
        self.noise_multiplier = self._compute_noise_multiplier()
        
        # ä¼˜åŒ–å™¨ï¼ˆæ·»åŠ  weight_decay ä½œä¸ºéšå¼ L2 æ­£åˆ™ï¼‰
        # weight_decay ä¼šè‡ªåŠ¨å¯¹æ‰€æœ‰å‚æ•°æ–½åŠ  L2 æƒ©ç½šï¼Œæ›´é«˜æ•ˆä¸”ä¸å½±å“æ¢¯åº¦è®¡ç®—
        self.optimizer = torch.optim.Adam(
            self.model.parameters(),
            lr=learning_rate,
            weight_decay=1e-5  # è½»é‡çº§ L2 æ­£åˆ™ï¼Œé˜²æ­¢å‚æ•°çˆ†ç‚¸
        )
        
        # è®­ç»ƒé˜¶æ®µé…ç½®
        self.phase_config = {
            'phase1': {
                'name': 'Frozen',
                'beta_l2': 1.0,
                'lr_scale': 0.0,  # å®Œå…¨å†»ç»“
            },
            'phase2': {
                'name': 'Conservative',
                'beta_l2': 0.8,  # å¼ºL2çº¦æŸ
                'lr_scale': 0.3,
            },
            'phase3': {
                'name': 'Moderate',
                'beta_l2': 0.5,  # ä¸­ç­‰çº¦æŸ
                'lr_scale': 0.7,
            },
            'phase4': {
                'name': 'Aggressive',
                'beta_l2': 0.2,  # å¼±çº¦æŸ
                'lr_scale': 1.0,
            }
        }
        
        print(f"\n{'='*60}")
        print(f"DP Sigmaé¢„æµ‹å™¨åˆå§‹åŒ–")
        print(f"{'='*60}")
        print(f"éšç§é¢„ç®—: Îµ={epsilon:.2f}, Î´={delta:.2e}")
        print(f"å™ªå£°å€æ•°: {self.noise_multiplier:.4f}")
        print(f"æ¢¯åº¦è£å‰ª: C={self.max_grad_norm}")
        print(f"å­¦ä¹ ç‡: {learning_rate:.2e}")
        print(f"æ€»æ­¥æ•°: {total_steps}")
        print(f"{'='*60}\n")
    
    def _compute_noise_multiplier(self) -> float:
        """
        è®¡ç®—æ»¡è¶³(Îµ,Î´)-DPçš„å™ªå£°å€æ•°ï¼ˆç†è®ºä¿è¯ï¼‰
        
        ä½¿ç”¨ä¸ Critic ç›¸åŒçš„ RDP ä¼šè®¡æ–¹æ³•ï¼š
        - é€šè¿‡ RDPAccountant ç´¯ç§¯éšç§æŸå¤±
        - äºŒåˆ†æœç´¢æ‰¾åˆ°æ»¡è¶³ç›®æ ‡ Îµ çš„æœ€å°å™ªå£°å€æ•°
        - batch_size=1ï¼ˆåœ¨çº¿å­¦ä¹ ï¼‰
        
        è¿™ç¡®ä¿äº†ä¸ Critic ä¸€è‡´çš„ç†è®ºä¿è¯ã€‚
        """
        # ä½¿ç”¨ä¸ Critic ç›¸åŒçš„æ–¹æ³•
        dp_result = compute_dpsgd_noise_multiplier(
            epsilon_target=self.epsilon,
            delta_target=self.delta,
            num_steps=self.total_steps,
            batch_size=1,  # Sigmaé¢„æµ‹å™¨æ˜¯åœ¨çº¿å­¦ä¹ ï¼ˆæ¯ä¸ªä»»åŠ¡ç‹¬ç«‹æ›´æ–°ï¼‰
            dataset_size=self.total_steps  # å‡è®¾æ¯æ­¥éƒ½æ˜¯æ–°æ ·æœ¬
        )
        
        noise_multiplier = dp_result['noise_multiplier']
        
        print(f"\n[DP-SGD ç†è®ºå‚æ•° - Sigmaé¢„æµ‹å™¨]")
        print(f"  æ€»æ­¥æ•° T: {self.total_steps}")
        print(f"  éšç§é¢„ç®— Îµ: {self.epsilon:.2f}")
        print(f"  å¤±è´¥æ¦‚ç‡ Î´: {self.delta:.2e}")
        print(f"  Batch Size: 1 (åœ¨çº¿å­¦ä¹ )")
        print(f"  é‡‡æ ·ç‡ q: {dp_result['sampling_rate']:.6f}")
        print(f"  æ¢¯åº¦è£å‰ª C: {self.max_grad_norm}")
        print(f"  å™ªå£°å€æ•° Ïƒ/C: {noise_multiplier:.4f}")
        print(f"  å™ªå£°æ ‡å‡†å·® Ïƒ: {noise_multiplier * self.max_grad_norm:.4f}")
        print(f"  ç†è®ºä¿è¯: ({self.epsilon:.2f}, {self.delta:.2e})-DP (RDPä¼šè®¡)")
        print(f"  æ–¹æ³•: ä¸Criticç›¸åŒçš„RDPä¼šè®¡\n")
        
        return noise_multiplier
    
    def get_current_phase(self, episode: int, total_episodes: int) -> Dict:
        """è·å–å½“å‰è®­ç»ƒé˜¶æ®µé…ç½®"""
        progress = episode / max(total_episodes, 1)
        
        if progress < 0.001:  # 0-25%
            phase = 'phase1'
        elif progress < 0.50:  # 25-50%
            phase = 'phase2'
        elif progress < 0.75:  # 50-75%
            phase = 'phase3'
        else:  # 75-100%
            phase = 'phase4'
        
        return self.phase_config[phase]
    
    def predict(self, features: torch.Tensor, requires_grad: bool = False) -> torch.Tensor:
        """
        é¢„æµ‹Sigmaå€¼
        
        Args:
            features: è¾“å…¥ç‰¹å¾ [B, D]
            requires_grad: æ˜¯å¦éœ€è¦æ¢¯åº¦ï¼ˆå¾®è°ƒæ—¶ä¸ºTrueï¼‰
        
        Returns:
            raw_logits: åŸå§‹è¾“å‡º [B, L]
        """
        if requires_grad:
            self.model.train()
            return self.model(features)
        else:
            self.model.eval()
            with torch.no_grad():
                return self.model(features)
    
    def compute_rule_based_sigma(self, 
                                 privacy_sensitivity: float,
                                 accuracy_requirement: float,
                                 sigma_min: float,
                                 sigma_max: float,
                                 task_node = None) -> float:
        """
        åŸºäºè§„åˆ™çš„Sigmaè®¡ç®—ï¼ˆç”¨ä½œæ­£åˆ™åŒ–åŸºçº¿ï¼‰
        
        ğŸ“Œ ä¸æ•°æ®ç”Ÿæˆå™¨çš„ _compute_comprehensive_target_sigma å®Œå…¨ä¸€è‡´
        è¿™æ˜¯é¢„è®­ç»ƒæ—¶ä½¿ç”¨çš„è§„åˆ™ï¼Œä¿æŒä½œä¸ºçº¦æŸ
        
        æ ¸å¿ƒè®¾è®¡ï¼š
        - Sigma ä¸éšç§æ•æ„Ÿåº¦å¼ºæ­£ç›¸å…³ï¼ˆä¸»å¯¼å› ç´ ï¼Œ60%ï¼‰
        - Sigma ä¸ç²¾åº¦éœ€æ±‚å¼ºè´Ÿç›¸å…³ï¼ˆä¸»å¯¼å› ç´ ï¼Œ40%ï¼‰
        - å…¶ä»–å› ç´ ä½œä¸ºå¾®è°ƒï¼ˆä»»åŠ¡ç±»å‹ã€DAGç»“æ„ç­‰ï¼Œåˆè®¡~15%ï¼‰
        
        Args:
            privacy_sensitivity: éšç§æ•æ„Ÿåº¦ [0, 1]
            accuracy_requirement: ç²¾åº¦éœ€æ±‚ [0, 1]
            sigma_min: sigmaä¸‹ç•Œ
            sigma_max: sigmaä¸Šç•Œ
            task_node: å¯é€‰çš„ä»»åŠ¡èŠ‚ç‚¹å¯¹è±¡ï¼ˆåŒ…å«æ›´å¤šå…ƒæ•°æ®ï¼‰
        """
        # è¶…å‚æ•°ï¼ˆä¸æ•°æ®ç”Ÿæˆå™¨ä¸€è‡´ï¼‰
        lambda_privacy = 5.0
        lambda_utility = 5.0
        temperature = 10.0
        target_scale_temp = 8.0
        epsilon = 1e-9
        
        # å½’ä¸€åŒ–è¾“å…¥
        privacy_sensitivity = float(np.clip(privacy_sensitivity, 0.0, 1.0))
        accuracy_requirement = float(np.clip(accuracy_requirement, 0.0, 1.0))
        
        # ========== æ ¸å¿ƒæƒé‡è®¡ç®— ==========
        # 5.1 éšç§æƒé‡ï¼ˆæ ¸å¿ƒ - å 60%å½±å“åŠ›ï¼‰
        privacy_weight = 1.0 + 4.0 * privacy_sensitivity  # [1.0, 5.0]
        
        # 5.2 ç²¾åº¦æƒé‡ï¼ˆæ ¸å¿ƒ - å 40%å½±å“åŠ›ï¼‰
        utility_weight = 1.0 + 3.0 * accuracy_requirement  # [1.0, 4.0]
        
        # ========== å¤šå› ç´ å¾®è°ƒï¼ˆå¦‚æœæä¾›äº†task_nodeï¼‰==========
        if task_node is not None:
            # 5.3 ä»»åŠ¡ç±»å‹è°ƒæ•´ï¼ˆå¾®è°ƒ - 10%å½±å“ï¼‰
            task_type = getattr(task_node, 'task_type', 'unknown')
            task_type_map = {
                'computation': 0.2,
                'communication': 0.4,
                'data_processing': 0.6,
                'sensing': 0.8,
                'unknown': 0.5
            }
            task_type_encoded = task_type_map.get(task_type, 0.5)
            type_privacy_bonus = task_type_encoded * 0.1  # [0, 0.08]
            privacy_weight = privacy_weight + type_privacy_bonus
            
            # 5.4 æˆªæ­¢æ—¶é—´å‹åŠ›è°ƒæ•´ï¼ˆå¾®è°ƒ - å‡å¼±å½±å“ï¼‰
            # ä»ä»»åŠ¡ç‰¹å¾ä¸­æå–ï¼ˆå¦‚æœç¯å¢ƒæœ‰æä¾›ï¼‰
            deadline_pressure = getattr(task_node, 'deadline_pressure', 0.5)
            deadline_pressure = float(np.clip(deadline_pressure, 0.0, 1.0))
            deadline_factor = np.exp(-1.0 * deadline_pressure)
            privacy_weight = privacy_weight * (0.9 + 0.1 * deadline_factor)
            
            # 5.5 ä¼˜å…ˆçº§è°ƒæ•´ï¼ˆå¾®è°ƒï¼‰
            priority = getattr(task_node, 'priority', 5.0)
            priority_normalized = float(np.clip(priority / 10.0, 0.0, 1.0))
            utility_weight = utility_weight * (1.0 + 0.1 * priority_normalized)
            
            # 5.6 DAGç»“æ„è°ƒæ•´ï¼ˆå¾®è°ƒï¼‰
            num_preds = len(getattr(task_node, 'pre', []))
            num_succs = len(getattr(task_node, 'suc', []))
            dag_complexity = (num_preds + num_succs) / 10.0
            dag_complexity = float(np.clip(dag_complexity, 0.0, 1.0))
            utility_weight = utility_weight * (1.0 + 0.05 * dag_complexity)
            
            # 5.7 å±‚çº§/è¿›åº¦è°ƒæ•´ï¼ˆå¦‚æœæœ‰è¿›åº¦ä¿¡æ¯ï¼‰
            # æ³¨æ„ï¼šè®­ç»ƒæ—¶å¯èƒ½æ²¡æœ‰å…¨å±€è¿›åº¦ä¿¡æ¯ï¼Œä½¿ç”¨å±‚çº§ä½œä¸ºä»£ç†
            layer = getattr(task_node, 'layer', 0)
            max_layers = 5  # å‡è®¾æœ€å¤§å±‚æ•°
            progress_proxy = layer / max(max_layers, 1)
            progress_proxy = float(np.clip(progress_proxy, 0.0, 1.0))
            progress_factor = 1.0 - 0.1 * (1.0 - progress_proxy)
            utility_weight = utility_weight * progress_factor
        
        # ========== è®¡ç®—æœ€ä¼˜Sigma ==========
        # ç»¼åˆæƒé‡
        A = lambda_privacy * privacy_weight
        B = lambda_utility * utility_weight
        
        # è®¡ç®—æ¯”ç‡
        ratio = A / (B + epsilon)
        ratio = float(np.clip(ratio, epsilon, 1e6))
        
        # å¯¹æ•°å˜æ¢
        s_optimal_raw = temperature * np.log(ratio)
        
        # æ˜ å°„åˆ° sigma èŒƒå›´
        s_mid = (sigma_max + sigma_min) / 2.0
        s_range = (sigma_max - sigma_min) / 2.0
        target_sigma = s_mid + s_range * np.tanh(s_optimal_raw / target_scale_temp)
        
        return float(np.clip(target_sigma, sigma_min, sigma_max))
    
    def dp_update_step(self,
                      features: torch.Tensor,
                      predicted_sigma: torch.Tensor,
                      reward: float,
                      phase_config: Dict,
                      rule_sigma: float = None) -> Dict:
        """
        å·®åˆ†ç§æœ‰çš„å•æ­¥æ›´æ–°ï¼ˆæ ‡å‡† DP-SGDï¼Œbatch_size=1ï¼‰
        
        Args:
            features: è¾“å…¥ç‰¹å¾ [1, D]ï¼ˆå•æ ·æœ¬ï¼‰
            predicted_sigma: é¢„æµ‹çš„sigmaå€¼ï¼ˆæ ‡é‡tensorï¼‰
            reward: å®é™…å¥–åŠ±ï¼ˆæ ‡é‡ï¼‰
            phase_config: å½“å‰é˜¶æ®µé…ç½®
            rule_sigma: è§„åˆ™åŸºçº¿sigmaï¼ˆç”¨äºæ­£åˆ™åŒ–ï¼‰
        
        Returns:
            æŸå¤±ä¿¡æ¯å­—å…¸
        
        DP-SGD æ­¥éª¤ï¼ˆç†è®ºä¿è¯ï¼‰ï¼š
        1. è®¡ç®—å•æ ·æœ¬æ¢¯åº¦: âˆ‡L(Î¸; x)
        2. è£å‰ªæ¢¯åº¦: g' = g / max(1, ||g||â‚‚/C)
        3. æ·»åŠ é«˜æ–¯å™ªå£°: gÌƒ = g' + N(0, ÏƒÂ²CÂ²I)
        4. æ›´æ–°å‚æ•°: Î¸ â† Î¸ - Î·Â·gÌƒ
        
        æ³¨æ„ï¼šbatch_size=1 æ—¶ï¼Œæ­¥éª¤2çš„å…¨å±€è£å‰ªç­‰ä»·äºper-sampleè£å‰ª
        """
 
        
        beta_l2 = phase_config['beta_l2']  # L2å‚æ•°æ­£åˆ™åŒ–æƒé‡ï¼ˆéšé˜¶æ®µåŠ¨æ€è°ƒæ•´ï¼‰
        lr_scale = phase_config['lr_scale']
        
        # å¦‚æœæ˜¯å†»ç»“é˜¶æ®µï¼Œç›´æ¥è¿”å›
        if lr_scale == 0.0:
            return {
                'total_loss': 0.0,
                'rl_loss': 0.0,
                'l2_loss': 0.0,
                'rule_loss': 0.0,
                'phase': phase_config['name']
            }
        
        self.model.train()
        
        # 1. RLæŸå¤±ï¼ˆè´Ÿå¥–åŠ±ï¼‰
        rl_loss = -torch.tensor(reward, dtype=torch.float32, device=self.device)
        
        # 2. L2å‚æ•°æ­£åˆ™åŒ–ï¼ˆç›¸å¯¹é¢„è®­ç»ƒå‚æ•°ï¼Œå¸¦æ—¶é—´è¡°å‡ï¼‰
        # ğŸ“Œ ä½¿ç”¨æ—¶é—´è¡°å‡ï¼šæ—©æœŸå¼ºçº¦æŸï¼ˆä¿æŒé¢„è®­ç»ƒçŸ¥è¯†ï¼‰ï¼ŒåæœŸå¼±çº¦æŸï¼ˆå…è®¸é€‚åº”ï¼‰
        # ğŸ“Œ é˜²æ­¢åœ¨çº¿å­¦ä¹ ä¸­ L2 æŸå¤±æ— é™å¢é•¿
        time_decay = max(0.0, 1.0 - self.current_step / self.total_steps)  # 1.0 â†’ 0.0
        l2_loss = torch.tensor(0.0, device=self.device)
        if time_decay > 0.01:  # åªåœ¨å‰99%çš„è®­ç»ƒä¸­ä½¿ç”¨ L2 æ­£åˆ™
            for name, param in self.model.named_parameters():
                if param.requires_grad and name in self.pretrained_params:
                    l2_loss += torch.sum((param - self.pretrained_params[name]) ** 2)
        
        # 3. è§„åˆ™æ­£åˆ™åŒ–æŸå¤±ï¼ˆä¿æŒä¸ªæ€§åŒ–ç­–ç•¥ï¼‰
        rule_loss = torch.tensor(0.0, device=self.device)
        if rule_sigma is not None:
            rule_target = torch.tensor(rule_sigma, dtype=torch.float32, device=self.device)
            # è½¯çº¦æŸï¼šä¸è¦æ±‚å®Œå…¨ç­‰äºè§„åˆ™ï¼Œä½†ä¸èƒ½åç¦»å¤ªè¿œ
            rule_loss = F.smooth_l1_loss(predicted_sigma.squeeze(), rule_target)
        
        # 4. ç»„åˆæŸå¤±ï¼ˆåŠ¨æ€æƒé‡å¹³è¡¡ï¼‰
        # 
        # æƒé‡è®¾è®¡åŸåˆ™ï¼š
        # 1. RLæŸå¤±æä¾›å­¦ä¹ ä¿¡å·ï¼ˆä¸»å¯¼ï¼‰
        # 2. L2æŸå¤±é˜²æ­¢ç¾éš¾æ€§é—å¿˜ï¼ˆä¸­ç­‰ï¼‰
        # 3. RuleæŸå¤±æä¾›è½¯çº¦æŸï¼ˆè¾…åŠ©ï¼‰
        #
        # åŸºäºè§‚å¯Ÿåˆ°çš„æŸå¤±æ•°é‡çº§ï¼š
        # - RL loss: ~2-8ï¼ˆå¥–åŠ±çš„è´Ÿå€¼ï¼‰
        # - L2 loss: ~3e5-3e6ï¼ˆå‚æ•°å¹³æ–¹å’Œï¼‰
        # - Rule loss: ~0.8-1.0ï¼ˆsmooth_l1_lossï¼‰
        #
        # ç›®æ ‡ï¼šè®©ä¸‰è€…çš„åŠ æƒè´¡çŒ®åœ¨åŒä¸€æ•°é‡çº§
        
        # åŠ¨æ€æƒé‡ï¼ˆéšè®­ç»ƒé˜¶æ®µè°ƒæ•´ï¼‰
        # Early: å¼ºçº¦æŸï¼ˆä¿æŒé¢„è®­ç»ƒç‰¹æ€§ï¼‰
        # Late: å¼±çº¦æŸï¼ˆå…è®¸é€‚åº”æ–°ä»»åŠ¡ï¼‰
        # 
        # æƒé‡ä¿®æ­£ï¼šåŸºäºå®é™…è§‚å¯Ÿåˆ°çš„æ•°å€¼
        # - RL loss (åŸå§‹): ~2-8 (è´Ÿå¥–åŠ±)
        # - L2 loss (åŸå§‹): ~0.001-0.002 (å‚æ•°å¹³æ–¹å’Œ)
        # - Rule loss (åŸå§‹): ~0.8-1.0 (smooth_l1)
        #
        # ç›®æ ‡è´¡çŒ®æ¯”ä¾‹: RLä¸»å¯¼(70%) > L2é˜²é—å¿˜(20%) > Ruleå¼•å¯¼(10%)
        # 
        # æ—¶é—´è¡°å‡ç­–ç•¥ï¼š
        # - L2 æƒé‡éšæ—¶é—´çº¿æ€§è¡°å‡åˆ° 0ï¼ˆé¿å…åœ¨çº¿å­¦ä¹ ä¸­æ— é™å¢é•¿ï¼‰
        # - Rule æƒé‡ä¿æŒç¨³å®šï¼ˆæŒç»­æä¾›è½¯çº¦æŸï¼‰
        weight_rl = 1.0                                          # RLä¿¡å·ï¼ˆä¸»å¯¼ï¼‰: è´¡çŒ® 2-8
        weight_l2 = beta_l2 * 1000.0 * time_decay               # L2æ­£åˆ™ï¼ˆæ—¶é—´è¡°å‡ï¼‰: è´¡çŒ® 0.8â†’0
        weight_rule = 0.3 * beta_l2                              # è§„åˆ™çº¦æŸï¼ˆè½¯å¼•å¯¼ï¼‰: è´¡çŒ® 0.24-0.3
        
        total_loss = (
            weight_rl * rl_loss                   # ä¸»å¯¼: ~2-8 (70-90%)
            + weight_l2 * l2_loss                 # è¾…åŠ©: ~0.8â†’0 (10-20%â†’0)
            + weight_rule * rule_loss             # å¼•å¯¼: ~0.24-0.3 (5-10%)
        )
        
        # 5. åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        total_loss.backward()
        
        # 6. æ¢¯åº¦è£å‰ªï¼ˆDP-SGDå…³é”®æ­¥éª¤ï¼‰
        total_norm = torch.nn.utils.clip_grad_norm_(
            self.model.parameters(),
            max_norm=self.max_grad_norm
        )
        
        # 7. æ·»åŠ é«˜æ–¯å™ªå£°ï¼ˆDP-SGDå…³é”®æ­¥éª¤ï¼‰
        for param in self.model.parameters():
            if param.grad is not None:
                noise = torch.normal(
                    mean=0.0,
                    std=self.noise_multiplier * self.max_grad_norm,
                    size=param.grad.shape,
                    device=param.grad.device,
                    dtype=param.grad.dtype
                )
                param.grad.add_(noise)
        
        # 8. æ›´æ–°å‚æ•°ï¼ˆç¼©æ”¾å­¦ä¹ ç‡ï¼‰
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = param_group['lr'] * lr_scale
        
        self.optimizer.step()
        
        # æ¢å¤åŸå§‹å­¦ä¹ ç‡
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = param_group['lr'] / lr_scale
        
        # æ›´æ–°æ­¥æ•°
        self.current_step += 1
        
        return {
            'total_loss': total_loss.item(),
            'rl_loss': rl_loss.item(),
            'l2_loss': l2_loss.item(),
            'rule_loss': rule_loss.item(),
            'grad_norm': total_norm.item(),
            'phase': phase_config['name'],
            'beta_l2': beta_l2,
            'lr_scale': lr_scale,
            # æ·»åŠ æƒé‡ä¿¡æ¯ç”¨äºç›‘æ§
            'weight_rl': weight_rl,
            'weight_l2': weight_l2,
            'weight_rule': weight_rule,
            'time_decay': time_decay  # ç›‘æ§æ—¶é—´è¡°å‡
        }
    
    def save(self, path: str):
        """ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'current_step': self.current_step,
            'epsilon': self.epsilon,
            'delta': self.delta,
        }, path)
        print(f"DP Sigmaé¢„æµ‹å™¨å·²ä¿å­˜åˆ°: {path}")
    
    def load(self, path: str):
        """åŠ è½½å¾®è°ƒåçš„æ¨¡å‹"""
        checkpoint = torch.load(path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.current_step = checkpoint['current_step']
        print(f"DP Sigmaé¢„æµ‹å™¨å·²ä» {path} åŠ è½½")


def _epsilon_single_gaussian_scan(sigma: float, sensitivity: float, delta: float) -> float:
    import math
    if sigma <= 0 or sensitivity <= 0:
        return float('inf')
    best = float('inf')
    s2 = sensitivity * sensitivity
    sig2 = sigma * sigma
    for a in _ALPHAS_EXT:
        if a <= 1.0:
            continue
        rho = a * s2 / (2.0 * sig2)
        eps = rho + math.log(1.0 / delta) / (a - 1.0)
        if eps < best:
            best = eps
    return float(best)


def set_seed(seed: int):
    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed); torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False

def create_dag(num_tasks: int = 5, max_layers: int = 3, seed: int = None) -> DAGTasks:
    task_config = {
        "cycles_range": (1e7, 1e8),
        "data_range": (1e5, 1e6),
        "deadline_base_range": (0.1, 0.5),
        "deadline_layer_offset": 0.015,
        "alpha_weights": (0.3, 0.3, 0.4),
    }
    dag = DAGTasks(num_tasks=num_tasks, max_layers=max_layers, seed=seed, task_config=task_config)
    return dag

def select_task_with_sigma(env: OffloadEnv, dp_sigma_predictor, params: Dict,
                           sigma_min: float, sigma_max: float,
                           enable_finetune: bool = False) -> Tuple[Optional[int], Optional[float], List[int], Optional[torch.Tensor], Optional[Dict]]:
    """
    ä½¿ç”¨DP Sigmaé¢„æµ‹å™¨æˆ–é™æ€å‡åˆ†ç­–ç•¥é€‰æ‹©ä»»åŠ¡å¹¶ç”Ÿæˆsigmaã€‚
    
    Args:
        env: ç¯å¢ƒ
        dp_sigma_predictor: DPSigmaPredictorå®ä¾‹ï¼ˆå¯ä¸ºNoneï¼‰
        params: å‚æ•°å­—å…¸
        sigma_min: sigmaæœ€å°å€¼
        sigma_max: sigmaæœ€å¤§å€¼
        enable_finetune: æ˜¯å¦å¯ç”¨å¾®è°ƒï¼ˆéœ€è¦è¿”å›é¢å¤–ä¿¡æ¯ï¼‰
    
    Returns:
        task_id: é€‰æ‹©çš„ä»»åŠ¡ID
        sigma: è¯¥ä»»åŠ¡çš„å™ªå£°å‚æ•°
        ready_tasks: å½“å‰å°±ç»ªçš„ä»»åŠ¡åˆ—è¡¨
        features: è¾“å…¥ç‰¹å¾ï¼ˆä»…enable_finetune=Trueæ—¶è¿”å›ï¼‰
        task_metadata: ä»»åŠ¡å…ƒæ•°æ®ï¼ˆä»…enable_finetune=Trueæ—¶è¿”å›ï¼‰
    """
    try:
        ready_tasks = env.get_ready_tasks()
        if not ready_tasks:
            return None, None, [], None, None
        
        task_id = ready_tasks[0]  # é€‰æ‹©ç¬¬ä¸€ä¸ªå°±ç»ªä»»åŠ¡
        node = env.dag.nodes.get(task_id)
        
        # æå–ä»»åŠ¡å…ƒæ•°æ®
        privacy_sens = float(getattr(node, 'privacy_sensitivity', 0.5))
        accuracy_req = float(getattr(node, 'accuracy_requirement', 0.5))
        
        # è®¡ç®—è§„åˆ™åŸºçº¿sigmaï¼ˆç”¨äºæ­£åˆ™åŒ–ï¼‰
        rule_sigma = None
        if dp_sigma_predictor is not None:
            rule_sigma = dp_sigma_predictor.compute_rule_based_sigma(
                privacy_sens, accuracy_req, sigma_min, sigma_max,
                task_node=node  # ä¼ å…¥å®Œæ•´çš„ä»»åŠ¡èŠ‚ç‚¹ä»¥è®¡ç®—å¤šå› ç´ æƒé‡
            )
        
        # å°è¯•ä½¿ç”¨Sigmaé¢„æµ‹å™¨
        if dp_sigma_predictor is not None and params.get("use_gace", False):
            try:
                inputs = env.prepare_transformer_inputs(ready_tasks)
                if inputs is not None:
                    tf, pf, df, adj, attn_mask, total_budget = inputs
                    
                    from transformer_alignment import ensure_self_loops_in_mask
                    device = dp_sigma_predictor.device
                    
                    tf = tf.to(device)
                    pf = pf.to(device)
                    df = df.to(device)
                    adj = adj.to(device)
                    attn_mask = ensure_self_loops_in_mask(attn_mask.to(device))
                    
                    # æ„é€ è¾“å…¥ç‰¹å¾
                    B, L = tf.shape[0], tf.shape[1]
                    base_features = torch.cat([tf, pf, df], dim=-1)  # [B, L, 25]
                    base_flat = base_features.reshape(B, -1)  # [B, L*25]
                    adj_flat = adj.reshape(B, -1)  # [B, L*L]
                    mask_flat = attn_mask.to(torch.float32).reshape(B, -1)  # [B, L*L]
                    
                    # æ·»åŠ å…ƒç‰¹å¾
                    meta_L = torch.full((B, 1), float(L), device=device, dtype=torch.float32)
                    meta_smin = torch.full((B, 1), sigma_min, device=device, dtype=torch.float32)
                    meta_smax = torch.full((B, 1), sigma_max, device=device, dtype=torch.float32)
                    meta_ps = torch.full((B, 1), privacy_sens, device=device, dtype=torch.float32)
                    meta_ar = torch.full((B, 1), accuracy_req, device=device, dtype=torch.float32)
                    
                    meta = torch.cat([meta_L, meta_smin, meta_smax, meta_ps, meta_ar], dim=1)
                    concatenated = torch.cat([base_flat, adj_flat, mask_flat, meta], dim=1)
                    
                    # é¢„æµ‹ï¼ˆæ ¹æ®æ˜¯å¦å¾®è°ƒå†³å®šæ˜¯å¦éœ€è¦æ¢¯åº¦ï¼‰
                    raw_logits = dp_sigma_predictor.predict(
                        concatenated, 
                        requires_grad=enable_finetune
                    )
                    
                    # æ‰¾åˆ°task_idåœ¨ready_tasksä¸­çš„ç´¢å¼•
                    try:
                        task_idx = ready_tasks.index(task_id)
                    except ValueError:
                        task_idx = 0
                    
                    # Tanhæ˜ å°„åˆ°sigmaèŒƒå›´
                    s_mid = (sigma_max + sigma_min) / 2.0
                    s_range = (sigma_max - sigma_min) / 2.0
                    sigma_pred_tensor = s_mid + s_range * torch.tanh(raw_logits[0, task_idx])
                    sigma_pred_clamped = torch.clamp(sigma_pred_tensor, min=sigma_min, max=sigma_max)
                    
                    # è½¬ä¸ºfloat
                    sigma = float(sigma_pred_clamped.item()) if not enable_finetune else sigma_pred_clamped
                    
                    # å¦‚æœå¯ç”¨å¾®è°ƒï¼Œè¿”å›é¢å¤–ä¿¡æ¯
                    if enable_finetune:
                        task_metadata = {
                            'privacy_sensitivity': privacy_sens,
                            'accuracy_requirement': accuracy_req,
                            'rule_sigma': rule_sigma,
                            'task_id': task_id,
                            'task_idx': task_idx
                        }
                        return task_id, sigma, ready_tasks, concatenated, task_metadata
                    else:
                        return task_id, sigma, ready_tasks, None, None
                        
            except Exception as e:
                print(f"[WARNING] Sigmaé¢„æµ‹å™¨æ¨æ–­å¤±è´¥: {e}ï¼Œä½¿ç”¨è§„åˆ™åŸºçº¿")
                import traceback
                traceback.print_exc()
                
                # å›é€€åˆ°è§„åˆ™åŸºçº¿
                if rule_sigma is not None:
                    return task_id, rule_sigma, ready_tasks, None, None
        
        # é™æ€å‡åˆ†ç­–ç•¥ï¼ˆé»˜è®¤ï¼‰
        sigma = (sigma_min + sigma_max) / 2.0
        
        # å¦‚æœæ²¡æœ‰é¢„æµ‹å™¨ä½†æœ‰è§„åˆ™ï¼Œä¼˜å…ˆä½¿ç”¨è§„åˆ™
        if rule_sigma is not None and dp_sigma_predictor is not None:
            sigma = rule_sigma
        
        return task_id, sigma, ready_tasks, None, None
        
    except Exception as e:
        print(f"[ERROR] select_task_with_sigma failed: {e}")
        ready_tasks = env.get_ready_tasks()
        if not ready_tasks:
            return None, None, [], None, None
        return ready_tasks[0], (sigma_min + sigma_max) / 2.0, ready_tasks, None, None


def prefill_replay_buffer(agents: List[DoubleDQNAgent], envs: List[OffloadEnv], params: Dict,
                          dp_sigma_predictor, min_samples: int, sigma_min: float, sigma_max: float):
    
    total_samples = 0
    for env in envs:
        env.reset()
    from tqdm import trange as _trange
    
    print(f"å¼€å§‹é¢„å¡«å…… {min_samples} æ¡ç»éªŒï¼ˆä»…Criticï¼‰...")
    
    with _trange(min_samples, desc="Prefill Buffer") as pbar:
        while total_samples < min_samples:
            for agent, env in zip(agents, envs):
                done = False
                step_count = 0
                max_steps = 200 # é™åˆ¶å•æ¬¡ episode çš„æœ€å¤§æ­¥æ•°
                
                while not done and step_count < max_steps and total_samples < min_samples:
                    # ä½¿ç”¨æ–°çš„sigmaç”Ÿæˆå‡½æ•°ï¼ˆé¢„å¡«å……æ—¶ä¸å¾®è°ƒï¼‰
                    task_id, sigma, ready_tasks, _, _ = select_task_with_sigma(
                        env, dp_sigma_predictor, params, sigma_min, sigma_max,
                        enable_finetune=False  # é¢„å¡«å……æ—¶ä¸å¾®è°ƒ
                    )
                    
                    if task_id is None:
                        env.reset()
                        break # DAG å®Œæˆæˆ–å‡ºé”™ï¼Œé‡ç½®ç¯å¢ƒ
                        
                    state = env.build_state(task_id)
                    
                    # å¦‚æœä½¿ç”¨transformerï¼Œå‡†å¤‡auxæ•°æ®
                    aux = None
                    if params.get("use_gace", False):
                        inputs = env.prepare_transformer_inputs(ready_tasks)
                        if inputs is not None:
                            tf, pf, df, adj, attn_mask, total_budget = inputs
                            aux = {
                                'task_features': tf, 'privacy_features': pf, 'dag_features': df,
                                'dag_adjacency': adj, 'attention_mask': attn_mask,
                                'total_budget': total_budget
                            }


                    # é¢„å¡«å……æ—¶ä½¿ç”¨éšæœºåŠ¨ä½œ
                    action = random.randint(0, 2)
                    dp_params = {"sigma": sigma, "q": 0.01}
                    next_state, reward, done, info = env.step(task_id, action, dp_params)

                    # å­˜å‚¨ Critic (DQN) çš„ç»éªŒ
                    agent.remember(state, action, reward, next_state, done, aux, task_id, sigma)
                    
                    total_samples += 1
                    step_count += 1
                    pbar.update(1)
                    
                    if total_samples >= min_samples: 
                        break
                        
                if done:
                    env.reset()
                    
                if total_samples >= min_samples: 
                    break
    
    print(f"Prefill done. Total Critic samples={total_samples}")

def main(override_params=None, exp_name=None):
    SEED = 42
    if override_params and 'seed' in override_params: SEED = int(override_params['seed'])
    set_seed(SEED)
    print(f"Seed set: {SEED}")

    params = {
        # ========== ç¯å¢ƒé…ç½® ==========
        "num_vehicles": 10,
        "num_tasks": 10,
        "min_tasks_per_episode": 5,
        "max_tasks_per_episode": 50,
        "max_layers": 3,

        # ========== ç‰©ç†å‚æ•° ==========
        "B_r": 1e6, "B_b": 2e6, "theta_vr": 0.5, "theta_vb": 0.5,
        "P_v": 0.5, "G_vr": 1e-5, "G_vb": 1e-6, "N_0": 1e-20, "I_vr": 1e-9, "backbone_bw": 1e9,
        "f_l": 1.5e9, "f_r": 3e10, "f_b": 2e11, "eta_vr": 0.2, "eta_vb": 0.1,

        # ========== ä»»åŠ¡é»˜è®¤å‚æ•° ==========
        "q_default": 0.01, "c_default": 1.0,
        "deadline_violation_penalty": 10.0,

        # ========== é¢„è®­ç»ƒSigmaé¢„æµ‹å™¨é…ç½® ==========
        "use_gace": True,  # æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒçš„sigmaé¢„æµ‹å™¨ï¼ˆåŸºäºå›¾æ„ŸçŸ¥ä¸Šä¸‹æ–‡ç¼–ç ï¼‰
        "pretrained_sigma_predictor_path": "transformer_sigma_allocator_for_rl.pth",  # é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ï¼ŒNoneåˆ™ä½¿ç”¨é™æ€å‡åˆ†ç­–ç•¥

        # ========== DDQNé…ç½® ==========
        "hidden_size": 256,
        "lr": 1e-5,
        "gamma": 0.99,
        "epsilon": 1.0,
        "epsilon_min": 0.01,
        "epsilon_decay": 0.99,
        "buffer_size": 50000,
        "batch_size": 64,

        # ========== è®­ç»ƒé…ç½® ==========
        "episodes": 800,
        "train_frequency": 1,  # æ¯Næ­¥è®­ç»ƒä¸€æ¬¡critic
        "sync_target_every_steps": 100,
        "prefill_steps": 7000,
        "max_grad_norm": 1.0,

        # ========== è”é‚¦å­¦ä¹ é…ç½® ==========
        "num_rsus": 5,
        "fl_aggregate_every_episodes": 10,

        # ========== æŸå¤±å¹³æ»‘é…ç½® ==========
        "loss_smoothing_window": 10,
        "use_ema_loss": True,
        "ema_beta": 0.9,

        # ========== å·®åˆ†éšç§é…ç½® ==========
        "use_opacus": True,  # ä½¿ç”¨Opacusè¿›è¡ŒDP-SGD
        "enable_budget_scaling": False,

        # ========== ç³»ç»Ÿé…ç½® ==========
        "seed": SEED,
        "device": "cuda" if torch.cuda.is_available() else "cpu",
    }

    if override_params:
        params.update({k: v for k, v in override_params.items()
                       if k not in ['exp_name', 'exp_description', 'output_dir', 'model_dir']})
        print(f"Applied override params")


    N_TOTAL_THEORETICAL = int(params["episodes"] * params["max_tasks_per_episode"])
    if N_TOTAL_THEORETICAL <= 0:
        raise ValueError("N_TOTAL_THEORETICAL must be positive. Check 'episodes' and 'max_tasks_per_episode'.")
    print(f"Privacy Theory: Theoretical Dataset Size (N_total) = {N_TOTAL_THEORETICAL}")
    # ---------------------------------------------------------------------

    # ç”Ÿæˆå®éªŒåç§°
    exp_name = f"{params['episodes']}iters_{EPSILON_TOTAL}eps_{params['num_vehicles']}vehicles_{params['max_tasks_per_episode']}tasks_fullmodel"
    exp_description = override_params.get('exp_description', '') if override_params else ''
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ç»Ÿä¸€ç›®å½•ç»“æ„ï¼šruns/experiments å’Œ model/experiments
    log_dir = os.path.join('runs/experiments', exp_name)
    checkpoint_dir = os.path.join('model/experiments', exp_name)
    os.makedirs(log_dir, exist_ok=True)
    os.makedirs(checkpoint_dir, exist_ok=True)

    # å†™å…¥é…ç½®æ–‡ä»¶
    with open(os.path.join(log_dir, 'config.txt'), 'w', encoding='utf-8') as f:
        f.write(f"è®­ç»ƒé…ç½® - {timestamp}\n")
        f.write(f"å®éªŒåç§°: {exp_name}\n")
        if exp_description:
            f.write(f"å®éªŒæè¿°: {exp_description}\n")
        f.write("=" * 60 + "\n")
        for k, v in params.items():
            f.write(f"{k}: {v}\n")
        f.write(f"dp_theoretical_dataset_size_N_total: {N_TOTAL_THEORETICAL}\n")

    print("=" * 60)
    if exp_name: print(f"å®éªŒåç§°: {exp_name}")
    if exp_description: print(f"å®éªŒæè¿°: {exp_description}")
    print(f"æ—¥å¿—ç›®å½•: {log_dir}\næ¨¡å‹ç›®å½•: {checkpoint_dir}")
    print(f"è®¾å¤‡: {params['device']}")
    print(f"å›¾æ„ŸçŸ¥ä¸Šä¸‹æ–‡ç¼–ç å™¨(GACE): {'å¯ç”¨' if params['use_gace'] else 'ç¦ç”¨'}")
    print(f"å•è½¦æ€»é¢„ç®—: Îµ_total={EPSILON_TOTAL}, Î´_total={DELTA_TOTAL}")
    print("=" * 60)

    dags = [create_dag(params["num_tasks"], params["max_layers"], seed=SEED+i) for i in range(params["num_vehicles"])]
    envs = [OffloadEnv(dag, params, device=params["device"]) for dag in dags]

    # è®¡ç®—é¢„ç®—åˆ†é…
    budgets = compute_level0_budgets(EPSILON_TOTAL, DELTA_TOTAL)
    eps_local = budgets["epsilon_local"]
    del_local = budgets["delta_local"]
    eps_critic = budgets["epsilon_critic"]
    del_critic = budgets["delta_critic"]
    eps_sigma_predictor = budgets["epsilon_sigma_predictor"]
    del_sigma_predictor = budgets["delta_sigma_predictor"]

    total_env_steps = params["episodes"] * params["max_tasks_per_episode"]* params["num_vehicles"]
    critic_real_steps = total_env_steps // params["train_frequency"]

    # Critic DP-SGDå™ªå£°è®¡ç®—
    critic_dp = compute_dpsgd_noise_multiplier(
        epsilon_target=eps_critic, delta_target=del_critic, num_steps=critic_real_steps,
        batch_size=params["batch_size"],
        dataset_size=N_TOTAL_THEORETICAL
    )
    print(f"[DP-SGD] Critic: Îµ={eps_critic:.2f}, Î´={del_critic:.2e}, z={critic_dp['noise_multiplier']:.4f}, q={critic_dp['sampling_rate']:.6f}")

    # åŠ è½½å¹¶åˆå§‹åŒ–DP Sigmaé¢„æµ‹å™¨
    dp_sigma_predictor = None
    pretrained_model_path = params.get("pretrained_sigma_predictor_path", None)
    
    if pretrained_model_path and os.path.exists(pretrained_model_path):
        print(f"\n{'='*60}")
        print(f"åŠ è½½é¢„è®­ç»ƒSigmaé¢„æµ‹å™¨: {pretrained_model_path}")
        print(f"{'='*60}")
        try:
            # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆä½¿ç”¨weights_only=Falseå› ä¸ºéœ€è¦åŠ è½½è‡ªå®šä¹‰æ¨¡å‹å¯¹è±¡ï¼‰
            # æ³¨æ„ï¼šä»…åœ¨ä¿¡ä»»æ¨¡å‹æ¥æºæ—¶ä½¿ç”¨
            pretrained_model = torch.load(
                pretrained_model_path, 
                map_location=params['device'],
                weights_only=False  # æ˜ç¡®æŒ‡å®šï¼Œæ¶ˆé™¤FutureWarning
            )
            
            # è®¡ç®—sigmaé¢„æµ‹å™¨çš„æ€»æ›´æ–°æ­¥æ•°
            # å‡è®¾æ¯ä¸ªepisodeå¹³å‡æ‰§è¡Œä¸€å®šæ•°é‡çš„ä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½å¯èƒ½æ›´æ–°sigmaé¢„æµ‹å™¨
            avg_tasks_per_episode = (params["min_tasks_per_episode"] + params["max_tasks_per_episode"]) / 2.0
            sigma_predictor_steps = int(params["episodes"] * avg_tasks_per_episode * params["num_vehicles"])
            
            # åˆå§‹åŒ–DP Sigmaé¢„æµ‹å™¨
            dp_sigma_predictor = DPSigmaPredictor(
                pretrained_model=pretrained_model,
                epsilon=eps_sigma_predictor,
                delta=del_sigma_predictor,
                total_steps=sigma_predictor_steps,
                learning_rate=params.get("sigma_predictor_lr", 1e-6),
                device=params['device'],
                params=params
            )
            print(f"âœ“ DP Sigmaé¢„æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
            print(f"  é¢„ç®—: Îµ={eps_sigma_predictor:.2f}, Î´={del_sigma_predictor:.2e}")
            print(f"  é¢„ä¼°æ›´æ–°æ­¥æ•°: {sigma_predictor_steps}")
            
        except Exception as e:
            print(f"âœ— Sigmaé¢„æµ‹å™¨åŠ è½½å¤±è´¥: {e}")
            print(f"  å°†ä½¿ç”¨é™æ€å‡åˆ†ç­–ç•¥")
            import traceback
            traceback.print_exc()
            dp_sigma_predictor = None
    else:
        print("\nâš  æœªæŒ‡å®šé¢„è®­ç»ƒSigmaé¢„æµ‹å™¨ï¼Œä½¿ç”¨é™æ€å‡åˆ†ç­–ç•¥")

    agents = []
    for i in range(params["num_vehicles"]):
        
        # [!! DP-FIX 3 !!]
        # ---------------------------------------------------------------------
        # å‡†å¤‡è¦ä¼ é€’ç»™ Agent çš„å‚æ•°ï¼ˆä¸å†åŒ…å«actorï¼‰
        agent_params = {
            **params, # å¤åˆ¶æ‰€æœ‰åŸºç¡€å‚æ•°
            # ä¼ å…¥è®¡ç®—å¥½çš„criticå™ªå£°
            "critic_noise_multiplier": critic_dp["noise_multiplier"],
            # ä¼ å…¥ç†è®ºä¸Šçš„æ•°æ®é›†å¤§å° N_total
            "dp_dataset_size": N_TOTAL_THEORETICAL
        }
        # å‡†å¤‡è¦ä¼ é€’ç»™ Agent çš„é¢„ç®— (ç”¨äºæ—¥å¿—è®°å½•æˆ– deltaï¼Œä»…critic)
        agent_budget = {
            "epsilon_critic": eps_critic,
            "delta_critic": del_critic,
        }

        agent = DoubleDQNAgent(
            state_size=15, 
            action_size=3, 
            params=agent_params, # <-- ä¼ å…¥ä¿®æ­£åçš„å‚æ•°
            device=params["device"], 
            per_vehicle_budget=agent_budget, # <-- ä¼ å…¥é¢„ç®—
            vehicle_id=i
        )
        # ---------------------------------------------------------------------
        agents.append(agent)

    fl = HierFL(num_vehicles=params["num_vehicles"], device=params["device"],
                num_rsus=params.get("num_rsus", 2))
    fl.set_global_model(agents[0])

    writer = SummaryWriter(log_dir=log_dir)

    for agent, env in zip(agents, envs):
        task_type_map = {task_idx: getattr(env.dag.nodes[task_idx], 'task_type', 'unknown') for task_idx in range(env.num_tasks)}
        agent.update_task_type_info(task_type_map)

    sigma_min_init, sigma_max_init = compute_sigma_range_for_episode(
        num_episodes=params["episodes"],
        tasks_in_episode=params["num_tasks"],
        epsilon_local=eps_local,
        delta_local=del_local,
        sensitivity=DELTA_SENS,
        max_tasks_per_episode=params["max_tasks_per_episode"] # ç¡®ä¿ä½¿ç”¨ M_max
    )
    print(f"åˆå§‹å™ªå£°èŒƒå›´ (åŸºäº M_max={params['max_tasks_per_episode']}): Ïƒ âˆˆ [{sigma_min_init:.4f}, {sigma_max_init:.4f}]")

    prefill_replay_buffer(agents=agents, envs=envs, params=params,
                          dp_sigma_predictor=dp_sigma_predictor,
                          min_samples=params["prefill_steps"],
                          sigma_min=sigma_min_init, sigma_max=sigma_max_init)

    global_step = 0
    critic_actual_steps = 0

    for ep in trange(params["episodes"], desc="Training"):
        for i, env in enumerate(envs):
            new_seed = SEED + ep * 1000 + i
            num_new_tasks = random.randint(params["min_tasks_per_episode"], params["max_tasks_per_episode"])
            new_dag = create_dag(num_tasks=num_new_tasks, max_layers=params["max_layers"], seed=new_seed)
            env.dag = new_dag; env.num_tasks = new_dag.num_tasks; env.reset()


        tasks_in_episode_M_max = params["max_tasks_per_episode"]
        sigma_min_ep, sigma_max_ep = compute_sigma_range_for_episode(
            num_episodes=params["episodes"],
            tasks_in_episode=tasks_in_episode_M_max, # <-- ä¿®å¤ï¼šä½¿ç”¨ M_max
            epsilon_local=eps_local, delta_local=del_local,
            sensitivity=DELTA_SENS,
            max_tasks_per_episode=tasks_in_episode_M_max # ç¡®ä¿ M_upper ä¹Ÿæ˜¯ M_max
        )
        # ---------------------------------------------------------------------
        
        writer.add_scalar("Episode/Sigma_Min_Target", sigma_min_ep, ep)
        writer.add_scalar("Episode/Sigma_Max_Target", sigma_max_ep, ep)

        episode_rewards = [0.0] * params["num_vehicles"]
        episode_losses = [[] for _ in range(params["num_vehicles"])]
        episode_sigma_losses = []  # Sigmaé¢„æµ‹å™¨å¾®è°ƒæŸå¤±
        
        # è·å–å½“å‰è®­ç»ƒé˜¶æ®µ
        if dp_sigma_predictor is not None:
            phase_config = dp_sigma_predictor.get_current_phase(ep, params["episodes"])
            enable_sigma_finetune = (phase_config['lr_scale'] > 0.0)
        else:
            phase_config = None
            enable_sigma_finetune = False
        
        # æ¯è½¦ä»»åŠ¡å®Œæˆç‡å’Œæ—¶å»¶ç»Ÿè®¡
        episode_task_stats = {
            "total_tasks": [0] * params["num_vehicles"],
            "task_rewards": [[] for _ in range(params["num_vehicles"])],      # æ–°å¢ï¼šæ¯ä»»åŠ¡å¥–åŠ±
            "end_to_end_delays": [[] for _ in range(params["num_vehicles"])], # ç«¯åˆ°ç«¯æ—¶å»¶ï¼ˆå«ç­‰å¾…ï¼‰
            "exec_times": [[] for _ in range(params["num_vehicles"])],        # æ‰§è¡Œæ—¶å»¶
            "waiting_delays": [[] for _ in range(params["num_vehicles"])],    # ç­‰å¾…æ—¶å»¶
            "deadline_violations": [0] * params["num_vehicles"],
            "sigmas": [[] for _ in range(params["num_vehicles"])],            # æ–°å¢ï¼šè®°å½•å®é™…ä½¿ç”¨çš„sigma
            "rule_sigmas": [[] for _ in range(params["num_vehicles"])],       # æ–°å¢ï¼šè®°å½•è§„åˆ™åŸºçº¿sigma
            "sigma_deviations": [[] for _ in range(params["num_vehicles"])]   # æ–°å¢ï¼šé¢„æµ‹ä¸è§„åˆ™çš„åå·®
        }

        for vehicle_id, (agent, env) in enumerate(zip(agents, envs)):
            done = False; step_count = 0; max_steps = 200
            while not done and step_count < max_steps:
                # ä½¿ç”¨DP Sigmaé¢„æµ‹å™¨ç”Ÿæˆsigmaï¼ˆæ”¯æŒå¾®è°ƒï¼‰
                task_id, sigma, ready_tasks, features, task_metadata = select_task_with_sigma(
                    env, dp_sigma_predictor, params, sigma_min_ep, sigma_max_ep,
                    enable_finetune=enable_sigma_finetune
                )
                if task_id is None: break
                state = env.build_state(task_id)
                
                # å‡†å¤‡auxæ•°æ®
                aux = None
                if params.get("use_gace", False):
                    inputs = env.prepare_transformer_inputs(ready_tasks)
                    if inputs is not None:
                        tf, pf, df, adj, attn_mask, total_budget = inputs
                        aux = {
                            'task_features': tf, 'privacy_features': pf, 'dag_features': df,
                            'dag_adjacency': adj, 'attention_mask': attn_mask,
                            'total_budget': total_budget
                        }

                action = agent.act(state, aux, sigma=sigma if not enable_sigma_finetune else float(sigma.item()))
                next_state, reward, done, info = env.step(task_id, action, {"sigma": sigma if not enable_sigma_finetune else float(sigma.item()), "q": 0.01})
                
                # ğŸ“Š Sigmaé¢„æµ‹å™¨DPå¾®è°ƒï¼ˆä»…åœ¨å…è®¸çš„é˜¶æ®µï¼‰
                if enable_sigma_finetune and features is not None and task_metadata is not None:
                    try:
                        loss_info = dp_sigma_predictor.dp_update_step(
                            features=features,
                            predicted_sigma=sigma,
                            reward=reward,
                            phase_config=phase_config,
                            rule_sigma=task_metadata.get('rule_sigma')
                        )
                        episode_sigma_losses.append(loss_info)
                        
                    except Exception as e:
                        print(f"[WARNING] Sigmaå¾®è°ƒå¤±è´¥: {e}")
                
                # æ”¶é›†ä»»åŠ¡æ‰§è¡Œç»Ÿè®¡ï¼ˆåŸºäºç¯å¢ƒå®é™…è¿”å›çš„å­—æ®µï¼‰
                episode_task_stats["total_tasks"][vehicle_id] += 1
                episode_task_stats["task_rewards"][vehicle_id].append(reward)  # è®°å½•æ¯ä»»åŠ¡å¥–åŠ±
                
                # è®°å½•sigmaç›¸å…³ç»Ÿè®¡
                sigma_val = sigma if not enable_sigma_finetune else float(sigma.item())
                episode_task_stats["sigmas"][vehicle_id].append(sigma_val)
                
                # å¦‚æœæœ‰è§„åˆ™åŸºçº¿ï¼Œè®°å½•åå·®
                if task_metadata is not None and 'rule_sigma' in task_metadata:
                    rule_sigma_val = task_metadata['rule_sigma']
                    episode_task_stats["rule_sigmas"][vehicle_id].append(rule_sigma_val)
                    episode_task_stats["sigma_deviations"][vehicle_id].append(abs(sigma_val - rule_sigma_val))
                
                if "end_to_end_delay" in info:
                    episode_task_stats["end_to_end_delays"][vehicle_id].append(info["end_to_end_delay"])
                if "exec_time" in info:
                    episode_task_stats["exec_times"][vehicle_id].append(info["exec_time"])
                if "waiting_delay" in info:
                    episode_task_stats["waiting_delays"][vehicle_id].append(info["waiting_delay"])
                if info.get("deadline_violated", False):
                    episode_task_stats["deadline_violations"][vehicle_id] += 1

                agent.remember(state, action, reward, next_state, done, aux, task_id, sigma)

                if global_step % params["train_frequency"] == 0 and len(agent.memory) >= params["batch_size"]:
                    loss, loss_info = agent.train_critic_step(task_id, noise_multiplier=agent.params["critic_noise_multiplier"])
                    episode_losses[vehicle_id].append(loss)
                    critic_actual_steps += 1
                    pool_sz = agent.memory.pool_size_for_task(task_id)
                    writer.add_scalar(f"Critic/PoolSize_Task{task_id}_V{vehicle_id}", pool_sz, global_step)
                    writer.add_scalar(f"DP/Critic_q_V{vehicle_id}_Task{task_id}", loss_info.get("dp/critic_q", 0.0), global_step)
                    writer.add_scalar(f"DP/Critic_z_V{vehicle_id}", loss_info.get("dp/critic_z", 0.0), global_step)

                if global_step % params["sync_target_every_steps"] == 0:
                    agent.update_target_network(hard=False, tau=0.001)

                episode_rewards[vehicle_id] += reward
                global_step += 1; step_count += 1

        # è”é‚¦å­¦ä¹ èšåˆï¼ˆä»…criticï¼‰
        if (ep + 1) % 5 == 0:
            fl.aggregate_models(agents)
            fl.distribute_model(agents)
        if (ep + 1) % params["fl_aggregate_every_episodes"] == 0:
            fl.aggregate_models(agents)
            fl.distribute_model(agents)

        # è®¡ç®—æ¯è½¦æ¯ä»»åŠ¡çš„å¹³å‡æŒ‡æ ‡
        # ä»»åŠ¡å®Œæˆç‡ = (æ€»ä»»åŠ¡æ•° - æˆªæ­¢æ—¶é—´è¿åæ•°) / æ€»ä»»åŠ¡æ•°
        total_tasks_all = sum(episode_task_stats["total_tasks"])
        total_violations = sum(episode_task_stats["deadline_violations"])
        
        # å…¨å±€å¹³å‡ä»»åŠ¡å®Œæˆç‡
        global_completion_rate = ((total_tasks_all - total_violations) / max(1, total_tasks_all)) * 100.0
        
        # å¹³å‡æ¯è½¦æ¯ä»»åŠ¡å®Œæˆç‡
        per_vehicle_completion_rates = []
        for v_id in range(params["num_vehicles"]):
            v_tasks = episode_task_stats["total_tasks"][v_id]
            v_violations = episode_task_stats["deadline_violations"][v_id]
            if v_tasks > 0:
                v_rate = ((v_tasks - v_violations) / v_tasks) * 100.0
                per_vehicle_completion_rates.append(v_rate)
        avg_per_vehicle_completion_rate = np.mean(per_vehicle_completion_rates) if per_vehicle_completion_rates else 0.0
        
        # æ—¶å»¶æŒ‡æ ‡è®¡ç®—
        # 1. ç«¯åˆ°ç«¯æ—¶å»¶ï¼ˆåŒ…æ‹¬ç­‰å¾…æ—¶é—´ï¼‰
        all_e2e_delays = [d for delays in episode_task_stats["end_to_end_delays"] for d in delays]
        global_avg_e2e_delay = np.mean(all_e2e_delays) if all_e2e_delays else 0.0
        
        per_vehicle_avg_e2e_delays = []
        for v_id in range(params["num_vehicles"]):
            v_delays = episode_task_stats["end_to_end_delays"][v_id]
            if v_delays:
                per_vehicle_avg_e2e_delays.append(np.mean(v_delays))
        avg_per_vehicle_e2e_delay = np.mean(per_vehicle_avg_e2e_delays) if per_vehicle_avg_e2e_delays else 0.0
        
        # 2. æ‰§è¡Œæ—¶å»¶ï¼ˆä»…ä»»åŠ¡æ‰§è¡Œæ—¶é—´ï¼‰
        all_exec_times = [t for times in episode_task_stats["exec_times"] for t in times]
        global_avg_exec_time = np.mean(all_exec_times) if all_exec_times else 0.0
        
        per_vehicle_avg_exec_times = []
        for v_id in range(params["num_vehicles"]):
            v_times = episode_task_stats["exec_times"][v_id]
            if v_times:
                per_vehicle_avg_exec_times.append(np.mean(v_times))
        avg_per_vehicle_exec_time = np.mean(per_vehicle_avg_exec_times) if per_vehicle_avg_exec_times else 0.0
        
        # 3. ç­‰å¾…æ—¶å»¶
        all_waiting_delays = [d for delays in episode_task_stats["waiting_delays"] for d in delays]
        global_avg_waiting_delay = np.mean(all_waiting_delays) if all_waiting_delays else 0.0
        
        per_vehicle_avg_waiting_delays = []
        for v_id in range(params["num_vehicles"]):
            v_delays = episode_task_stats["waiting_delays"][v_id]
            if v_delays:
                per_vehicle_avg_waiting_delays.append(np.mean(v_delays))
        avg_per_vehicle_waiting_delay = np.mean(per_vehicle_avg_waiting_delays) if per_vehicle_avg_waiting_delays else 0.0
        
        # 4. Sigmaç»Ÿè®¡
        all_sigmas = [s for sigmas in episode_task_stats["sigmas"] for s in sigmas]
        global_avg_sigma = np.mean(all_sigmas) if all_sigmas else 0.0
        global_std_sigma = np.std(all_sigmas) if all_sigmas else 0.0
        global_min_sigma = np.min(all_sigmas) if all_sigmas else 0.0
        global_max_sigma = np.max(all_sigmas) if all_sigmas else 0.0
        
        per_vehicle_avg_sigmas = []
        for v_id in range(params["num_vehicles"]):
            v_sigmas = episode_task_stats["sigmas"][v_id]
            if v_sigmas:
                per_vehicle_avg_sigmas.append(np.mean(v_sigmas))
        avg_per_vehicle_sigma = np.mean(per_vehicle_avg_sigmas) if per_vehicle_avg_sigmas else 0.0
        
        # æ¯ä»»åŠ¡å¹³å‡å¥–åŠ±
        all_task_rewards = [r for rewards in episode_task_stats["task_rewards"] for r in rewards]
        global_avg_task_reward = np.mean(all_task_rewards) if all_task_rewards else 0.0
        
        per_vehicle_avg_task_rewards = []
        for v_id in range(params["num_vehicles"]):
            v_rewards = episode_task_stats["task_rewards"][v_id]
            if v_rewards:
                per_vehicle_avg_task_rewards.append(np.mean(v_rewards))
        avg_per_vehicle_task_reward = np.mean(per_vehicle_avg_task_rewards) if per_vehicle_avg_task_rewards else 0.0

        avg_reward = np.mean(episode_rewards)
        avg_loss = np.mean([np.mean(losses) if losses else 0 for losses in episode_losses])
        
        # è®°å½•episodeçº§åˆ«å¥–åŠ±ï¼ˆæ¯è½¦ç´¯è®¡ï¼‰
        writer.add_scalar("reward/avg_episode_reward", avg_reward, ep)
        
        # è®°å½•ä»»åŠ¡çº§åˆ«å¥–åŠ±ï¼ˆæ¯ä»»åŠ¡å¹³å‡ï¼‰
        writer.add_scalar("reward/global_avg_task_reward", global_avg_task_reward, ep)
        writer.add_scalar("reward/avg_per_vehicle_task_reward", avg_per_vehicle_task_reward, ep)
        
        # è®°å½•ä»»åŠ¡å®Œæˆç‡æŒ‡æ ‡
        writer.add_scalar("performance/global_task_completion_rate", global_completion_rate, ep)
        writer.add_scalar("performance/avg_per_vehicle_completion_rate", avg_per_vehicle_completion_rate, ep)
        writer.add_scalar("performance/deadline_violation_count", total_violations, ep)
        
        # è®°å½•ç«¯åˆ°ç«¯æ—¶å»¶æŒ‡æ ‡ï¼ˆå«ç­‰å¾…æ—¶é—´ï¼‰
        writer.add_scalar("delay/global_avg_end_to_end_delay", global_avg_e2e_delay, ep)
        writer.add_scalar("delay/avg_per_vehicle_end_to_end_delay", avg_per_vehicle_e2e_delay, ep)
        
        # è®°å½•æ‰§è¡Œæ—¶å»¶æŒ‡æ ‡ï¼ˆä»…æ‰§è¡Œæ—¶é—´ï¼‰
        writer.add_scalar("delay/global_avg_exec_time", global_avg_exec_time, ep)
        writer.add_scalar("delay/avg_per_vehicle_exec_time", avg_per_vehicle_exec_time, ep)
        
        # è®°å½•ç­‰å¾…æ—¶å»¶æŒ‡æ ‡
        writer.add_scalar("delay/global_avg_waiting_delay", global_avg_waiting_delay, ep)
        writer.add_scalar("delay/avg_per_vehicle_waiting_delay", avg_per_vehicle_waiting_delay, ep)
        
        # è®°å½•Sigmaç»Ÿè®¡æŒ‡æ ‡ï¼ˆå®é™…ä½¿ç”¨å€¼ï¼‰
        writer.add_scalar("sigma/global_avg", global_avg_sigma, ep)
        writer.add_scalar("sigma/global_std", global_std_sigma, ep)
        writer.add_scalar("sigma/global_min", global_min_sigma, ep)
        writer.add_scalar("sigma/global_max", global_max_sigma, ep)
        writer.add_scalar("sigma/avg_per_vehicle", avg_per_vehicle_sigma, ep)
        writer.add_scalar("sigma/range_utilization", 
                         (global_max_sigma - global_min_sigma) / max(0.01, sigma_max_ep - sigma_min_ep) if all_sigmas else 0.0, 
                         ep)
        
        # ğŸ“Š Sigmaé¢„æµ‹å™¨ vs è§„åˆ™åŸºçº¿å¯¹æ¯”åˆ†æ
        all_rule_sigmas = [s for rule_sigmas in episode_task_stats["rule_sigmas"] for s in rule_sigmas]
        all_sigma_deviations = [d for deviations in episode_task_stats["sigma_deviations"] for d in deviations]
        
        if all_rule_sigmas:
            global_avg_rule_sigma = np.mean(all_rule_sigmas)
            global_std_rule_sigma = np.std(all_rule_sigmas)
            
            writer.add_scalar("sigma_comparison/avg_predicted_sigma", global_avg_sigma, ep)
            writer.add_scalar("sigma_comparison/avg_rule_sigma", global_avg_rule_sigma, ep)
            writer.add_scalar("sigma_comparison/sigma_diff", global_avg_sigma - global_avg_rule_sigma, ep)
        
        if all_sigma_deviations:
            global_avg_deviation = np.mean(all_sigma_deviations)
            global_max_deviation = np.max(all_sigma_deviations)
            global_std_deviation = np.std(all_sigma_deviations)
            
            writer.add_scalar("sigma_comparison/avg_deviation", global_avg_deviation, ep)
            writer.add_scalar("sigma_comparison/max_deviation", global_max_deviation, ep)
            writer.add_scalar("sigma_comparison/deviation_ratio", 
                            global_avg_deviation / max(global_avg_sigma, 1e-9), ep)
        
        # ğŸ“Š Sigmaé¢„æµ‹å™¨å¯¹éšç§-æ•ˆç”¨æƒè¡¡çš„å½±å“
        if all_sigmas and all_task_rewards:
            # è®¡ç®—sigmaä¸å¥–åŠ±çš„ç›¸å…³æ€§ï¼ˆPearsonç›¸å…³ç³»æ•°ï¼‰
            if len(all_sigmas) == len(all_task_rewards) and len(all_sigmas) > 1:
                try:
                    # æ£€æŸ¥æ ‡å‡†å·®æ˜¯å¦ä¸º0ï¼ˆé¿å…é™¤ä»¥0è­¦å‘Šï¼‰
                    sigma_std = np.std(all_sigmas)
                    reward_std = np.std(all_task_rewards)
                    
                    if sigma_std > 1e-9 and reward_std > 1e-9:
                        # æ ‡å‡†å·®éé›¶ï¼Œå¯ä»¥å®‰å…¨è®¡ç®—ç›¸å…³ç³»æ•°
                        sigma_reward_corr = np.corrcoef(all_sigmas, all_task_rewards)[0, 1]
                        # æ£€æŸ¥æ˜¯å¦æ˜¯NaN
                        if not np.isnan(sigma_reward_corr):
                            writer.add_scalar("sigma_predictor/analysis/sigma_reward_correlation", sigma_reward_corr, ep)
                    else:
                        # æ ‡å‡†å·®ä¸º0ï¼Œç›¸å…³ç³»æ•°æ— æ„ä¹‰ï¼Œè®°å½•ä¸º0
                        writer.add_scalar("sigma_predictor/analysis/sigma_reward_correlation", 0.0, ep)
                except Exception as e:
                    # è®¡ç®—å¤±è´¥ï¼Œé™é»˜å¤„ç†
                    pass
            
            # é«˜sigma vs ä½sigmaçš„å¥–åŠ±å¯¹æ¯”
            median_sigma = np.median(all_sigmas)
            high_sigma_rewards = [r for s, r in zip(all_sigmas, all_task_rewards) if s >= median_sigma]
            low_sigma_rewards = [r for s, r in zip(all_sigmas, all_task_rewards) if s < median_sigma]
            
            if high_sigma_rewards and low_sigma_rewards:
                writer.add_scalar("sigma_predictor/analysis/high_sigma_avg_reward", np.mean(high_sigma_rewards), ep)
                writer.add_scalar("sigma_predictor/analysis/low_sigma_avg_reward", np.mean(low_sigma_rewards), ep)
                writer.add_scalar("sigma_predictor/analysis/reward_gap_by_sigma", 
                                np.mean(high_sigma_rewards) - np.mean(low_sigma_rewards), ep)
        
        writer.add_scalar("loss/critic_avg", avg_loss, ep)
        writer.add_scalar("epsilon/value", agents[0].epsilon, ep)
        
        # ğŸ“Š Sigmaé¢„æµ‹å™¨å¾®è°ƒæ—¥å¿—ï¼ˆè¯¦ç»†è§‚æµ‹ï¼‰
        if episode_sigma_losses:
            # === åŸºç¡€æŸå¤±ç»Ÿè®¡ ===
            avg_sigma_total_loss = np.mean([x['total_loss'] for x in episode_sigma_losses])
            avg_sigma_rl_loss = np.mean([x['rl_loss'] for x in episode_sigma_losses])
            avg_sigma_l2_loss = np.mean([x['l2_loss'] for x in episode_sigma_losses])
            avg_sigma_rule_loss = np.mean([x['rule_loss'] for x in episode_sigma_losses])


            # === è®­ç»ƒé˜¶æ®µä¿¡æ¯ ===
            current_phase = episode_sigma_losses[0]['phase']
            current_beta_l2 = episode_sigma_losses[0]['beta_l2']
            
            # è·å–å®é™…ä½¿ç”¨çš„æƒé‡ï¼ˆæ–°æƒé‡æ–¹æ¡ˆï¼‰
            weight_rl = episode_sigma_losses[0].get('weight_rl', 1.0)
            weight_l2 = episode_sigma_losses[0].get('weight_l2', current_beta_l2 * 1e-6)
            weight_rule = episode_sigma_losses[0].get('weight_rule', 0.5 * current_beta_l2)
            
            phase_idx = ['frozen', 'conservative', 'moderate', 'aggressive'].index(current_phase.lower())
            
            # === ç»„åˆæŸå¤±æ¯”ä¾‹åˆ†æï¼ˆä½¿ç”¨å®é™…æƒé‡ï¼‰===
            total_rl_contribution = avg_sigma_rl_loss * weight_rl
            total_l2_contribution = avg_sigma_l2_loss * weight_l2
            total_rule_contribution = avg_sigma_rule_loss * weight_rule
            total_weighted = total_rl_contribution + total_l2_contribution + total_rule_contribution
            
            rl_ratio = total_rl_contribution / max(total_weighted, 1e-9)
            l2_ratio = total_l2_contribution / max(total_weighted, 1e-9)
            rule_ratio = total_rule_contribution / max(total_weighted, 1e-9)
            
            # === æŸå¤±å˜åŒ–ç‡ï¼ˆä¸ä¸Šä¸€episodeå¯¹æ¯”ï¼‰===
            # éœ€è¦å…¨å±€å˜é‡å­˜å‚¨ä¸Šä¸€episodeçš„æŸå¤±ï¼Œè¿™é‡Œå…ˆè®¡ç®—å½“å‰å€¼
            
            # 1. æŸå¤±å‡å€¼
            writer.add_scalar("sigma_predictor/loss/total_loss", avg_sigma_total_loss, ep)
            writer.add_scalar("sigma_predictor/loss/rl_loss", avg_sigma_rl_loss, ep)
            writer.add_scalar("sigma_predictor/loss/l2_loss", avg_sigma_l2_loss, ep)
            writer.add_scalar("sigma_predictor/loss/rule_loss", avg_sigma_rule_loss, ep)
            
            # 6. æŸå¤±ç»„æˆæ¯”ä¾‹ï¼ˆè¯Šæ–­å“ªä¸ªæŸå¤±é¡¹å ä¸»å¯¼ï¼‰
            writer.add_scalar("sigma_composition/composition/rl_contribution", total_rl_contribution, ep)
            writer.add_scalar("sigma_composition/composition/l2_contribution", total_l2_contribution, ep)
            writer.add_scalar("sigma_composition/composition/rule_contribution", total_rule_contribution, ep)
            writer.add_scalar("sigma_composition/composition/rl_ratio", rl_ratio, ep)
            writer.add_scalar("sigma_composition/composition/l2_ratio", l2_ratio, ep)
            writer.add_scalar("sigma_composition/composition/rule_ratio", rule_ratio, ep)
            

            
            # 8. åŠ æƒæŸå¤±å¯¹æ¯”ï¼ˆéªŒè¯æƒé‡è®¾è®¡ - ä½¿ç”¨å®é™…æƒé‡ï¼‰
            writer.add_scalar("sigma_predictor/weighted_loss/weighted_rl", weight_rl * avg_sigma_rl_loss, ep)
            writer.add_scalar("sigma_predictor/weighted_loss/weighted_l2", weight_l2 * avg_sigma_l2_loss, ep)
            writer.add_scalar("sigma_predictor/weighted_loss/weighted_rule", weight_rule * avg_sigma_rule_loss, ep)
            
            # 9. æƒé‡æœ¬èº«çš„ç›‘æ§ï¼ˆè§‚å¯ŸåŠ¨æ€å˜åŒ–ï¼‰
            writer.add_scalar("sigma_predictor/weights/weight_rl", weight_rl, ep)
            writer.add_scalar("sigma_predictor/weights/weight_l2", weight_l2, ep)
            writer.add_scalar("sigma_predictor/weights/weight_rule", weight_rule, ep)


        for agent in agents: agent.decay_epsilon()
        
    print(f"\n{'='*60}")
    print(f"è®­ç»ƒå®Œæˆ - DP-SGD æ­¥æ•°éªŒè¯:")
    print(f"{'='*60}")
    print(f"Critic (æ‰€æœ‰DPSGDé¢„ç®—):")
    print(f"  é¢„ä¼°æ­¥æ•°: {critic_real_steps}")
    print(f"  å®é™…æ­¥æ•°: {critic_actual_steps}")
    print(f"  å·®å¼‚ç‡: {abs(critic_actual_steps - critic_real_steps) / max(1, critic_real_steps) * 100:.2f}%")
    if critic_actual_steps > critic_real_steps:
        print(f"\nâš ï¸  è­¦å‘Š: Criticå®é™…æ­¥æ•° ({critic_actual_steps}) è¶…è¿‡é¢„ä¼° ({critic_real_steps})! éšç§é¢„ç®—å·²è¶…æ”¯!")
    print(f"{'='*60}\n")

    os.makedirs(checkpoint_dir, exist_ok=True)
    for i, agent in enumerate(agents):
        if hasattr(agent.q_network, "_module"):
            torch.save(agent.q_network._module.state_dict(), os.path.join(checkpoint_dir, f'agent_critic_{i}_final.pth'))
        else:
            torch.save(agent.q_network.state_dict(), os.path.join(checkpoint_dir, f'agent_critic_{i}_final.pth'))
    
    # ğŸ’¾ ä¿å­˜å¾®è°ƒåçš„Sigmaé¢„æµ‹å™¨
    if dp_sigma_predictor is not None:
        sigma_save_path = os.path.join(checkpoint_dir, 'sigma_predictor_finetuned.pth')
        dp_sigma_predictor.save(sigma_save_path)
        print(f"\nâœ… Sigmaé¢„æµ‹å™¨å¾®è°ƒæ¨¡å‹å·²ä¿å­˜: {sigma_save_path}")
        print(f"   éšç§é¢„ç®—: Îµ={budgets['epsilon_sigma_predictor']:.2f}, Î´={budgets['delta_sigma_predictor']:.2e}")
        print(f"   æ€»å¾®è°ƒæ­¥æ•°: {sigma_predictor_steps}")
        print(f"   å™ªå£°ä¹˜æ•°: {dp_sigma_predictor.noise_multiplier:.4f}\n")

    writer.close()
    return 0.0

def parse_args():
    import argparse
    parser = argparse.ArgumentParser(description='è®­ç»ƒå±‚æ¬¡åŒ–è”é‚¦å¼ºåŒ–å­¦ä¹ æ¨¡å‹')
    parser.add_argument('--config', type=str, default=None, help='YAMLé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--exp_name', type=str, default='default', help='å®éªŒåç§°')
    parser.add_argument('--exp_description', type=str, default='', help='å®éªŒæè¿°')
    parser.add_argument('--output_dir', type=str, default='runs/new', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--model_dir', type=str, default='model/new', help='æ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--use_gace', type=lambda x: x.lower() == 'true', default=None, help='æ˜¯å¦ä½¿ç”¨å›¾æ„ŸçŸ¥ä¸Šä¸‹æ–‡ç¼–ç å™¨')
    parser.add_argument('--episodes', type=int, default=None)
    parser.add_argument('--num_tasks', type=int, default=None)
    parser.add_argument('--num_rsus', type=int, default=None)
    parser.add_argument('--seed', type=int, default=None)
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_args()
    override_params = {}
    if args.config:
        import yaml
        print(f"åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
        with open(args.config, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        override_params.update(config)
    if args.use_gace is not None: override_params['use_gace'] = args.use_gace
    if args.episodes is not None: override_params['episodes'] = args.episodes
    if args.num_tasks is not None: override_params['num_tasks'] = args.num_tasks
    if args.num_rsus is not None: override_params['num_rsus'] = args.num_rsus
    if args.seed is not None: override_params['seed'] = args.seed
    override_params['exp_name'] = args.exp_name if args.exp_name != 'default' else override_params.get('exp_name', 'default')
    override_params['exp_description'] = args.exp_description or override_params.get('exp_description', '')
    override_params['output_dir'] = args.output_dir if args.output_dir != 'runs/new' else override_params.get('output_dir', 'runs/new')
    override_params['model_dir'] = args.model_dir if args.model_dir != 'model/new' else override_params.get('model_dir', 'model/new')
    main(override_params)